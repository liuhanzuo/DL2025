# Variational Auto Encoder(VAE)

*How to design a model that is easy to sample? and easy to compute loglikelihood?*

使用隐变量$z$来生成更高维的变量$x$,本质上要做的是一个概率模型

$$
p(x,z)=p(z)p(x|z)
$$

一些概念:

* $p(z)$: prior distribution
* $p(x|z)$:conditional distribution
* $p(z|x)$: posterior distribution
* $p(x)=\int_z p(x|z)p(z)$: marginal distribution

假设: $p(x)$仍然是一个很复杂的概率分布,但是$p(z)$和$p(x|z)$可以简单地sample到(事实上,后面会说明他们都是Gaussian分布)

## Latent Variable Model

Gaussian Mixture Model 聚类模型,将一些点用聚类表示, 可以表示为:

$$
z\sim \text{Categorical}(w_1,\cdots,w_k), x\sim N(\mu_Z,\Sigma_Z)
$$

训练: 学习聚类中心,方差; 生成: sample 一个$z$, 根据聚类分布生成$x$

$$
L(\theta)=\log\prod_z p_\theta(x)=\sum_{x\in D}\log \sum_z p_\theta(x,z)
$$

使用能量模型

$$
p(x,z)\sim\exp(-E(x,z))
$$

可以使得$\nabla L(\theta)$易于计算

**Importance Sampling**

$$
p(x)=\sum_z q(z)\frac{p_\theta(x,z)}{q(z)},L(x,\theta)=\log p_\theta(x)=\log\sum_z q(z)\frac{p_\theta(x,z)}{q(z)}
$$

From Jensen Ineq.,

$$
L(x,\theta)\ge\sum_z q(z)\log\frac{p_\theta(x,z)}{q(z)}
$$

这就是**ELBO(Evidence Lower Bound)**! 目标:$q(z)\approx p_\theta(z|x)$

* 根据loss $L(\theta)$更新$\theta$
* 让$q(z)=p_\theta(x,z)$

所以, 我们需要一个神经网络来逼近$p_\theta$的概率分布

> 这里我的理解是$q_\phi$是用来学习显示输入$x$对应的概率分布 -- 如果理解为图像的话, 就是学习图像对映的特征(概率分布), 让$q_\phi(z)$的概率分布逼近于给定ground truth($x$)下的概率分布

最小化:$KL(q||p)=\sum_z q(z)\log\frac{q(z)}{p(z)}$

$$
KL(q(z;\phi)||p(z|x))=\sum_zq_\phi(z)\log\frac{q_\phi(z)}{p(z|x)}=\sum_zq_\phi(z)\log\frac{q_\phi(z)p(x)}{p(z,x)}\\
=\sum_z q_\phi(z)\log p(x)-\sum_z q_\phi(z)\log\frac{q_\phi(z)}{p(z,x)}=\log p(x)-\sum_z q_\phi(z)\log\frac{q_\phi(z)}{p(z,x)}
$$

我们定义

$$
L(\phi)=\sum_z q_\phi(z)\log\frac{p(z,x)}{q_\phi(z)}
$$

这样$L(\phi)$其实就是ELBO

$$
\log p(x)=KL(q_\phi(z)||p(z|x))+\sum_zq_\phi(z)\log\frac{q_\phi(z)p(x)}{p(z,x)}=\text{approx error}+ELBO\ge ELBO
$$

最终的loss(Amortized loss is called $J(\theta,\phi;x)$)

$$
J(\theta,\phi;x)=L(\phi,\theta)=\sum_z q_\phi(z)\log\frac{p_\theta(x,z)}{q_\phi(z)}\to L(\phi,\theta)=\sum_z q_\phi(z|x)\log\frac{p_\theta(x,z)}{q_\phi(z|x)}\\
=\sum_z q_\phi(z|x)(\log p_\theta(x|z)-\log q_\phi(z|x)+\log p_\theta(z))\\
=\sum_z q_\phi(z|x)\log p_\theta(x|z)-\sum_z q_\phi(z|x)\log\frac{q_\phi(z|x)}{p_\theta(z)}\\
=E_{z\sim q_\phi(\cdot)}[\log p_\theta(x|z)]-KL(q_\phi(z|x)||p_\theta(z))
$$

为了方便计算: 我们如此选取概率分布: 这里$f_{i,j},\mu,\sigma$都是神经网络

$$
p(z)\sim N(0,I)\\
p_\theta(x_{i,j}|z)\sim N(f_{i,j,\theta}(z),1)\\
q_\phi(z|x)\sim N(\mu_\phi(x),diag(\exp(\sigma_\phi(x))))
$$

loss由两项构成,后面的KL divergence项对gaussian有closed form的表达式

考虑标记/未标记的数据 如何考虑有标签的数据?

$$
D_l=\{(x^i,y^i)\},D_u=\{(x^i)\}
$$

Decoder: $p_\theta(x|y,z)$, Encoder: $q_\phi(z,y|x)\to q_\phi(z|x)\cdot q_\phi(y|x)$

$$
KL=KL(p(z)||q(z))+KL(p(y)||q(y)), p(y)\sim Uniform\\
Reconstruction~Loss =E_{z,y\sim q(z,y)}[\log p_\theta(x|z,y)]=E_{\epsilon\sim N(0,1),y\sim q(y)}[log p_\theta(x|\mu(x)+\sigma(x)\cdot\epsilon,y)]
$$
