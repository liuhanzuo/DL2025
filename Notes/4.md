# EBM(Energy Based Model)

## Hopfield Network

一个有圈的网络, 每个点属于$\{1,-1\}$,权重矩阵$W_{i,j}$,其中$w_{i,j}=w_{j,i}$

更新方案:

$$
y_i=\Theta\left(\sum_{j\neq i}w_{j,i}y_j+b_i\right),\Theta(z)=sign(z)
$$

更新一定会停止:

$$
D(y_1,\cdots,y_N)=\sum_{j<i}y_iw_{i,j}y_j+y_ib_i,\Delta D>0
$$

能量被定义为:

$$
E=-D
$$

所以...本质上EBM其实是使用一个网络$W_{i,j}$记录了所有需要的图像信息,让能量最低的点记录所有能量信息,生成的时候,只需要随机初始化网络,然后依次更新所有节点,就可以得到一个local optimum!

Hebbian Learning Rule;

$$
W_{i,j}=\frac{1}{N_p}\sum_p y_i^py_j^p, P=\{y^p\}\text{是ground truth, 有}N_p\text{个patterns}
$$

 研究表明:最多存储的个数$N_p\le \frac{N}{4}\log N$

但是,记录的图像可能会导致其他原本不希望被记录的位置也有较低的能量,从而导致找到并非记住的位置

为了解决这个问题,我们需要提高*fake memory*的energy

$$
W=\arg\min_W\sum_{y\in P}E(y)-\sum_{y'\notin P}E(y')\\
W\leftarrow W-\eta\left(\sum_{y\in P}yy^T-\sum_{y'\notin P\& y'\in Valley}y'y'^T\right)
$$

在需要记忆的pattern周围的y'更重要

> 最终的更新模式:
>
> $$
> W\leftarrow W-\eta\left(\mathbb{E}_{y\in P}[yy^T]-\mathbb{E}_{y'}[y'y'^T]\right)
> $$
>
> $y'$被随机初始化成所需的pattern,然后随机更新$y'$几步(2~4步)

扩充的元素个数:使用latent nuerons, 并不可见,但也可更新

## Boltzman Machine

Boltzman Probability

$$
P(y)=\frac{1}{Z}\exp(-E(y))=\frac{1}{Z}\exp\left(\sum_{i<j}w_{i,j}y_iy_j+b_iy_i\right)
$$

### Stochastic Hopfield Network

$P(y)$ 是给定$E(y)$ 状态下$y$的稳态分布

> $$
> z_i=\sum_j w_{i,j}y_j+2b_i\\
> P(y_i=1|y_{j\neq i})=\frac{1}{1+\exp(-z_i)}=\sigma(z_i)
> $$

> 更新状态:$y$,$y'$只在$i$上不同,$y_i=1,y_i'=-1$
>
> $$
> \log P(y)=-E(y)+C\\
> E(y)=-\sum_{i<j}w_{i,j}y_iy_j-b_iy_i\\
> \log P(y)-\log P(y')=E(y')-E(y)=-\sum_{j}w_{i,j}y_j-2b_i
> $$

如何生成一个状态$y$?

根据更新规则sample$y_0,\cdots,y_L$, 实际输出是:

$$
y_i=I[\frac{1}{M}\sum_{t=L-M+1}^L y_i(t)]>0
$$

在实际应用中,可以仅仅取$M=1$的结果(直接输出$y_L$)

> Temperature Annealing: 从高温度出发,逐渐降低温度
>
> Initialize $y_0$, $T\leftarrow T_{max}$
>
> repeat:$y_i(T)\sim Bernoulli(\sigma(\frac{1}{T}Z_i(T)))$ 也就是说$P(y_i=1|\{y_j\}_{j\neq i})=\sigma(\frac 1 T z_i)$
>
> $y_i(\alpha T)\leftarrow y_i(T); T\leftarrow \alpha T$

$$
E(y)=-\frac 1 2 y^TWy\\
P(y)=\frac 1 Z \exp(-\frac{E(y)} T)\\
P(y)=\frac{\exp(\frac 1 2 y^TWy)}{\sum_{y'}\exp(\frac 1 2 y'^T W y)}\\
L(W)=\frac{1}{N_P}\sum_{y\in P}\frac 1 2 y^TWy-\log\sum_{y'}\exp(\frac 1 2 y'^T Wy')
$$

$$
\nabla_{W_{i,j}}L=\frac{1}{N_P}\sum_{y\in P}y_iy_j-\sum_{y'}\frac{\exp(\frac 1 2y'^TWy')}Z\cdot y_i'y_j'\\
=\frac{1}{N_P}\sum_{y\in P}y_iy_j-\mathbb{E}_{y'}[y_i'y_j']=\frac 1 {|P|}\sum_{v\in P}\mathbb{E}_{h}[y_iy_j]-\mathbb{E}_{y'}[y_i'y_j']
$$

以下我们使用Monte-Carlo Approximation 来估计这里的这个期望.

我们选取$S$作为sample的集合,共有$M$个元素,这样我们有$\mathbb{E}_{y'}[y_i'y_j']=\frac{1}{M}\sum_{y'\in S}y_i'y_j'$

### Restricted Boltzman Machine

$$
\begin{cases}z_i=\sum_j w_{i,j}v_j, P(h_i=1|v_i)=\frac 1 {1+\exp(-z_i)}\text{ for hidden neurons }h_i\\
z_j=\sum_i w_{i,j}h_i, P(v_j=1|h_i)=\frac 1 {1+\exp(-z_j)}\text{ for visible neurons }v_j  
\end{cases}
$$

运行Gibbs Sampling, 从$v_0$出发, 依次sample$v_0$,$h_0$,$v_1$,$\cdots$, 只需要sample三次即可得到较为准确的估计!

### Deep Boltzman Machine

让整个网络加深,有多层隐藏层, 从visible layer到最深的隐藏层依次更新(forward),backward的时候反向

> 平均场近似推断. 这里计算方便起见,我们假设后验分布$Q(h|v)$可以因式分解:
>
> $$
> Q(h|v)=\prod_{l=1}^L\prod_j q(h_j^l),q(h_j^l=1)=\mu_j^l
> $$
>
> 在迭代更新中我们通过更新$\mu_j^l$来逼近后验概率
>
> $$
> \mu_j^l=\sigma\left(\sum_iW_{i,j}^{l-1}\mu_i^{l-1}+\sum_kW_{j,k}^l\mu_k^{l+1}+b_j^l\right)
> $$

> Gibbs 交替采样
>
> $$
> P(h_j^l=1|h^{l-1},h^{l+1})=\sigma\left(\sum_i W_{i,j}^{l-1}h_i^{l-1}+\sum_kW_{j,k}^{l}h_{k}^{l+1}+b_j^l\right)
> $$

> 训练过程:输入数据,得到隐藏层$< h_i^lh_j^{l+1}>_{data}=\mu_i^l\mu_j^{l+1}$.直接从模型中采样,得到$<h_i^lh_j^{l+1}>_{model}$
>
> $$
> \Delta W=\eta\left(<h_i^lh_j^{l+1}>_{data}-<h_i^lh_j^{l+1}>_{model}\right)
> $$

### Energy Based Model

$$
P(x)=\frac{1}{Z}\exp(-E_\theta(x))\\
L(\theta)=\log P(x)=-E_\theta(x)-\log Z(\theta)\\
\nabla_\theta L(\theta)\approx\nabla_\theta(-E_\theta(x_{train})+E_\theta(x_{sample}))
$$

**Rejection Sampling**

首先构造$Q(x)$

$$
E_{x\sim P(x)}[f(x)]=E_{x\sim q(x)}[\frac{p(x)}{q(x)}f(x)]
$$

**MCMC(Markov Chain Monte-Carlo)**

设计一个马尔可夫过程使得稳态分布就是需要的分布:让$\pi$就是稳态分布,需要满足:

* Detailed Balance

$$
\pi(s)T(s\to s')=\pi(s')T('s\to s)
$$

* Unique Stationary Distribution -- Markove Chain is ergodic

$$
\min_z\min_{z':\pi(z')>0}\frac{T(z\to z')}{\pi(z')}=\delta>0
$$

如果这些都满足,可以使用Mestropolis Hastings Algorithm

* proposal distribution $q(s'\to s)$
* draw $s'\sim q(s'\to s)$
* $$
  \alpha=\min(1,\frac{p(s')q(s'\to s)}{p(s)q(s\to s')})
  $$
* Transition to $s'$ is accepted with ratio $\alpha$(if re ject, stay at $s$)

**Metropolis Hastings Algorithm**

* Random Initialize $s^0$
* $$
  s'\leftarrow s+noise
  $$
* Transition to $s'$ with probability $\min(1,\frac{p(s')}{p(s)})$(otherwise stay at $s$)

问题:如果acceptance ratio很低怎么办? 如何找到一个转移概率使得$q(s\to s')$总是被接受?

**Gibbs Sampling**

$$
X_i^{t+1}\sim P(X_i|X_{-i}^t)
$$

*Gibbs Sampling's Acceptance Rate is always 1*

update policy

注意到:

$$
p(\theta|X)\sim p(\theta)\prod_{i=1}^N p(x_i|\theta)\\
\nabla \log p(\theta|X)=\nabla\log p(\theta)+\sum_{i=1}^N \nabla_\theta\log p(x_i|\theta)
$$

使用小批量数据估计: (整个数据集是$D$, mini-batch是$M$)

$$
\nabla \log p(\theta|X)=\nabla \log p(\theta)+|D|\mathbb{E}_{x\sim D}[\nabla_\theta \log p(x_i|\theta)]\\
=\log p(\theta)+\frac{|D|}{|M|}\sum_{i=1}^M[\nabla_\theta \log p(x_i|\theta)]
$$
