# Normalizing Flow

对于一个双射$f:x\to z$, $z=f^{-1}(x)$, 我们有:

$$
p(x)=p(f^{-1}(x))|det\left(\frac{\partial f^{-1}(x)}{\partial x}\right)|=p(z)|det\left(\frac{\partial f^{-1}(x)}{\partial x}\right)|
$$

这样,我们构造$K$个双射,$z_0\to z_1\to\cdots\to z_K=x$

$$
p(z_i)=p(z_{i-1})|det(\frac{\partial f_i}{\partial z_{i-1}})|^{-1}
$$

$$
\log p(x)=\log p(z_0)-\sum_i\log|det(\frac{\partial f_i}{\partial z_{i-1}})|
$$

但是计算Jacobian项需要$O(d^3)$复杂度,我们需要设计好的$f_i$使得Jacobian容易计算!

## NICE

**Forward Pass $x\to z$**

$$
z_{1:m}=z_{1:m},z_{m+1:d}=z_{m+1:d}-\mu_\theta(z_{1:m})\\
J=\frac{\partial f}{\partial x}, detJ=1
$$

为了保证最终不是unit-volume transformation,第一层的NICE可以使用对角线上非1的项

$$
J=diag(S)\\
det J=\prod_i S_{i,i}
$$

由于只有一部分$z_i$可以改变,实际上可以看作$x=(x_v,x_h)$的可见项和隐藏项

## Real-NVP

**Forward Pass $x\to z$**

$$
z_{1:m}=z_{1:m},z_{m+1:d}=x_{m+1:d}\cdot\exp(\alpha_\theta(z_{1:m}))+\mu_\theta(x_{1:m})
$$

其中$\mu_\theta,\alpha_\theta$是神经网络

**Backward Pass $z\to x$**

$$
x_{1:m}=z_{1:m}, x_{m+1:d}=(z_{m+1:d}-\mu_\theta(z_{1:m}))\cdot\exp(-\alpha_\theta(x_{1:m}))\\
det J=\prod_{i=m+1}^d\exp(\alpha_\theta(x_{1:m})_i)
$$

## GLOW

核心观点: Jacobian矩阵的大小和channel大小的三次方正相关,可以通过1*1 convolutions来降低channel大小,从而使$\log|\det J|=h\cdot w\cdot \log|\det W|\in O(c^3)$

容易计算

# Score-based Generative Model

$$
q(x_t|x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t I),q(x_{1:T}|x_0)=\prod_{t=1}^T q(x_t|x_{t-1})
$$

## DDPM(Denoising  Diffusion Probabilistic Models)

Original loss

$$
L_t=\mathbb{E}_{x_o,\epsilon}\left[\frac{(1-\alpha_t)^2}{2\alpha_t(1-\overline{\alpha_t})|\Sigma_\theta|_2^2}|\epsilon_t-\epsilon_\theta(\sqrt{\overline\alpha_t}x_0+\sqrt{1-\overline\alpha_t}\epsilon_t,t))|\right]
$$

Now: **Training Process**

$$
\nabla_\theta\|\epsilon-\epsilon_\theta(\sqrt{\overline{\alpha_t}}x_0+\sqrt{1-\overline{\alpha_t}}\epsilon_t,t)\|^2,x_0\sim q(x_0),t\sim Uniform(\{1,\cdots,T\}),\epsilon\sim N(0,1)
$$

**Sampling Process**

$$
x_{t-1}=\frac 1 {\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\overline{\alpha_t}}}\epsilon_\theta(x_t,t)\right)+\sigma_t z,x_T\sim N(0,1),t=T,\cdots,1,z\sim N(0,1)(t>1)
$$

## Score-Based Model

如何表示一个概率分布 $p(x$)? $s(x)=\nabla_x \log p(x)$, 最小化fisher divergence

$$
\frac 1 2\mathbb{E}_{x\sim p_{data}}[|\nabla_x\log p_{data}(x)-\nabla_x\log p(x)_2^2|]\\
=\mathbb{E}_{x\sim p_{data}}[\frac 1 2|\nabla_x\log p_\theta(x)|_2^2+tr(\nabla_x^2\log p_\theta(x))]+Const
$$

所以,score based model不需要解决配分函数$Z$的问题(EBM使用MCMC进行sample的部分)

问题转化为,对于一个概率分布$p$,如何sample出它的score function呢

$$
=\mathbb{E}_{x\sim p_{data}}[\frac 1 2|s(x)|_2^2+tr(s(x))]+Const
$$

中,第一项好算,第二项难以计算($O(d)$个back-propogation).

我们发现,可以通过在原有概率分布上增加一点点噪音来算出对于噪音概率分布的score-function,数学见PPT

$$
\frac 1 2\mathbb{E}_{\tilde x\sim q_\sigma}\left[\|\nabla_{\tilde{x}}\log q_\sigma(\tilde{x})-s_\theta(\tilde{x})\|_2^2\right]\\
=Const+\frac 1 2\mathbb{E}_{x\sim p_{data}(\cdot),\tilde{x}\sim q_\sigma(\cdot|x)}[\|s_\theta(\tilde{x})-\nabla_{\tilde{x}}\log q_\sigma(\tilde{x}|x)\|_2^2]
$$

由于$\tilde{x}$的condition probability分布很容易计算,所以上面这个式子很容易计算

* 如果$\sigma$过小,则方差很大
* 如果$\sigma$过大,则概率分布被加了一个很大的噪音,score function不准确

Pitfalls:

* manifold hypothesis: 有很大的空间内部没有数据点,这些地方的$\nabla_x\log p_{data}(x)$无法被定义
* challenge in low data density regions. 在低密度数据的区域 由于没有数据,score function无法被sample到,导致不准确
* slow mixing of Langevin dynamics between data modes 对于不同模态的概率分布,Langevin dynamics无法学习他们的混合比例(由于只学习了sample的数据)

Solution:

**Gaussian pertubation!**

加入高斯噪声,三个问题都可以被解决!

注意需要从大到小逐渐降低variance,multi-scale noise scales

$$
L(\theta) = \frac 1 L \sum_{i=1}^L \lambda(\sigma_i) \mathbb{E}_{q_{\sigma_i}(x)} \left[\|s_\theta(\tilde{x}, \sigma_i) - \nabla_{\tilde{x}} \log q_{\sigma_i}(\tilde{x}|x)\|_2^2\right]\\
=\frac 1 L \sum_{i=1}^L \lambda(\sigma_i) \mathbb{E}_{x \sim p_{data}} \mathbb{E}_{\tilde{x} \sim q_{\sigma_i}(\tilde{x}|x)} \left[\|s_\theta(\tilde{x}, \sigma_i) - \nabla_{\tilde{x}} \log q_{\sigma_i}(\tilde{x}|x)\|_2^2\right]+Const\\
=\frac 1 L\sum_{i=1}^L \lambda(\sigma_i)\mathbb{E}_{x\sim p_{data},z\sim N(0,1)}\left[\|s_\theta(x+\sigma_iz,\sigma_i)+\frac{z}{\sigma_i}\|_2^2\right]+Const
$$

* 选择$\sigma_1$是最大的数据点间距离,$\sigma_L$充分小来让最终的noise足够逼近
* $\lambda(\sigma_i)=\sigma_i^2$

$$
\frac 1 L\sum_{i=1}^L \lambda(\sigma_i)\mathbb{E}_{x\sim p_{data},z\sim N(0,1)}\left[\|s_\theta(x+\sigma_iz,\sigma_i)+\frac{z}{\sigma_i}\|_2^2\right]\\
=\frac 1 L\sum_{i=1}^L\mathbb{E}_{x\sim p_{data},z\sim N(0,1)}\left[\|\epsilon_\theta(x+\sigma_iz,\sigma_i)+z\|_2^2\right]
$$

在forward的过程中,实际上就是infinite distribution 的过程,注意这里的连续形式和之前的离散形式是等价的,只不过一般的diffusion model中我们都使用固定的步数,比如DDPM

$$
dx=f(x,t)dt+\sigma(t)dw
$$

$$
dx=\sigma(t)dw\to_{time~interval}dx=-\sigma^2(t)\nabla_x\log p_t(x)+\sigma(t)d\tilde{w}
$$

Sampling: Euler-Maruyama

$$
x\leftarrow x-\sigma(t)^2s_\theta(x,t)\Delta t+\sigma(t)z(z\sim N(0,|\Delta t|I)), t\leftarrow t+\Delta t
$$

C**onsistency Model:**

learn$f_\theta(x_t,t)\to x_0$

**Controllable Model**

$$
y\to x_0\to\cdots\to x_T\\
dx=-\sigma^2(t)\nabla_x\log p_t(x|y)dt+\sigma(t)dw=-\sigma^2(t)[\nabla_x\log p_t(x)+\nabla_x\log p_t(y|x)]dt+\sigma(t)dw
$$

这里独立出来了一个time-dependent classifier,这一项可以被单独取出进行训练,也就是说可以先训练一个unlabelled-diffusion model,再加上一个classifier就可以完成有label的diffusion
